{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8454f2f-73cb-4c81-9d2d-aa79ccbd36fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "\n",
      "Data loaded successfully!\n",
      "Train data shape: (18524, 9)\n",
      "Test data shape: (6175, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <th>Stage_fear</th>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <th>Going_outside</th>\n",
       "      <th>Drained_after_socializing</th>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <th>Post_frequency</th>\n",
       "      <th>Personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
       "0   0               0.0         No                      6.0            4.0   \n",
       "1   1               1.0         No                      7.0            3.0   \n",
       "2   2               6.0        Yes                      1.0            0.0   \n",
       "3   3               3.0         No                      7.0            3.0   \n",
       "4   4               1.0         No                      4.0            4.0   \n",
       "\n",
       "  Drained_after_socializing  Friends_circle_size  Post_frequency Personality  \n",
       "0                        No                 15.0             5.0   Extrovert  \n",
       "1                        No                 10.0             8.0   Extrovert  \n",
       "2                       NaN                  3.0             0.0   Introvert  \n",
       "3                        No                 11.0             5.0   Extrovert  \n",
       "4                        No                 13.0             NaN   Extrovert  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/raw/train.csv')\n",
    "    test_df = pd.read_csv('../data/raw/test.csv')\n",
    "    sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "\n",
    "    print(\"\\nData loaded successfully!\")\n",
    "    print(f\"Train data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'train.csv', 'test.csv', and 'sample_submission.csv' are in the 'data/raw/' directory.\")\n",
    "    # Exit or handle gracefully if files aren't found\n",
    "    # For now, we'll assume they are found to proceed with instructions\n",
    "test_ids = test_df['id'].copy() if test_df is not None and 'id' in test_df.columns else None\n",
    "display(train_df.head())\n",
    "\n",
    "features = ['id', 'Time_spent_Alone', 'Stage_fear', 'Social_event_attendance',\n",
    "       'Going_outside', 'Drained_after_socializing', 'Friends_circle_size',\n",
    "       'Post_frequency', 'Personality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2906102c-2791-4d07-8cbf-7a03e8909787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                              0\n",
      "Time_spent_Alone             1190\n",
      "Stage_fear                   1893\n",
      "Social_event_attendance      1180\n",
      "Going_outside                1466\n",
      "Drained_after_socializing    1149\n",
      "Friends_circle_size          1054\n",
      "Post_frequency               1264\n",
      "Personality                     0\n",
      "dtype: int64\n",
      "id                             0\n",
      "Time_spent_Alone             425\n",
      "Stage_fear                   598\n",
      "Social_event_attendance      397\n",
      "Going_outside                466\n",
      "Drained_after_socializing    432\n",
      "Friends_circle_size          350\n",
      "Post_frequency               408\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().sum())\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afa1d269-f040-46e8-b5a2-c0480c972f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to store fitted imputers and encoders\n",
    "# This allows them to be used across different function calls (e.g., for train and then for validation/test)\n",
    "global_numerical_imputer = None\n",
    "global_categorical_imputers = {} # Dictionary to store imputers for each categorical column\n",
    "global_categorical_encoders = {} # Dictionary to store encoders for each categorical column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10367a2a-a8db-4ba0-a67f-a8ca0838bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_transform_train_data(X_train_df):\n",
    "    \"\"\"\n",
    "    Fits imputers and encoders on the training data and returns the transformed training data.\n",
    "    Assumes 'id' column and target column are already removed from X_train_df.\n",
    "    \"\"\"\n",
    "    X_train_processed = X_train_df.copy()\n",
    "\n",
    "    # Identify columns based on their data types\n",
    "    numerical_cols = X_train_processed.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Categorical columns are typically 'object' or 'category' dtype\n",
    "    categorical_cols = X_train_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # --- Numerical Imputation ---\n",
    "    if numerical_cols:\n",
    "        # Fit numerical imputer on training data\n",
    "        global global_numerical_imputer\n",
    "        global_numerical_imputer = SimpleImputer(strategy='median')\n",
    "        global_numerical_imputer.fit(X_train_processed[numerical_cols])\n",
    "        # Transform training data\n",
    "        X_train_processed[numerical_cols] = global_numerical_imputer.transform(X_train_processed[numerical_cols])\n",
    "\n",
    "    # --- Categorical Imputation and Encoding ---\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            # Impute missing categorical values with the mode of the training data\n",
    "            imputer = SimpleImputer(strategy='most_frequent')\n",
    "            imputer.fit(X_train_processed[[col]]) # Fit only on this column\n",
    "            X_train_processed[[col]] = imputer.transform(X_train_processed[[col]])\n",
    "            global_categorical_imputers[col] = imputer # Store the fitted imputer\n",
    "\n",
    "            # Encode categorical values using OrdinalEncoder\n",
    "            encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1) # Handles unseen categories in test data\n",
    "            encoder.fit(X_train_processed[[col]]) # Fit only on this column\n",
    "            X_train_processed[[col]] = encoder.transform(X_train_processed[[col]])\n",
    "            global_categorical_encoders[col] = encoder # Store the fitted encoder\n",
    "\n",
    "    return X_train_processed\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89c71b2e-50df-4bf2-b2c7-8f80ebdbac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(X_data_df):\n",
    "    \"\"\"\n",
    "    Transforms new data (validation/test) using the globally fitted imputers and encoders.\n",
    "    Assumes 'id' column and target column are already removed from X_data_df.\n",
    "    \"\"\"\n",
    "    X_data_processed = X_data_df.copy()\n",
    "\n",
    "    # Identify columns (should be same as training data)\n",
    "    numerical_cols = X_data_processed.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = X_data_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # --- Numerical Imputation ---\n",
    "    if numerical_cols and global_numerical_imputer:\n",
    "        # Use the imputer fitted on training data to transform new data\n",
    "        X_data_processed[numerical_cols] = global_numerical_imputer.transform(X_data_processed[numerical_cols])\n",
    "    elif numerical_cols and not global_numerical_imputer:\n",
    "        print(\"Warning: Numerical imputer not fitted. Run 'fit_and_transform_train_data' first.\")\n",
    "\n",
    "    # --- Categorical Imputation and Encoding ---\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            if col in global_categorical_imputers and col in global_categorical_encoders:\n",
    "                # Use imputers and encoders fitted on training data\n",
    "                imputer = global_categorical_imputers[col]\n",
    "                encoder = global_categorical_encoders[col]\n",
    "\n",
    "                X_data_processed[[col]] = imputer.transform(X_data_processed[[col]])\n",
    "                X_data_processed[[col]] = encoder.transform(X_data_processed[[col]])\n",
    "            else:\n",
    "                print(f\"Warning: Imputer/Encoder not found for categorical column '{col}'. It might not have been in training data or preprocessing was skipped.\")\n",
    "                # Fallback: if not found, fillna and try to encode (might create issues if new categories are not handled by encoder)\n",
    "                # For simplicity, we'll just print warning. In real code, you might want a more robust fallback.\n",
    "                X_data_processed[col] = X_data_processed[col].fillna(X_data_processed[col].mode()[0]) # Basic fill if no imputer\n",
    "                # If encoder not found, this will fail or pass through original object type if not handled.\n",
    "                # It's crucial that test data columns match train data columns and types.\n",
    "\n",
    "    return X_data_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e1e274c-0329-4777-819c-2dcbbd962678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of X_train_final_processed (NumPy array after all steps):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.57735027,  1.71808243, -0.15329284, -0.13483997,  0.04607757,\n",
       "        -0.57735027, -0.57735027, -0.45749597, -0.57735027, -0.27304851,\n",
       "        -0.57419554, -0.57265629, -0.37796447,  1.32434032, -0.2173781 ,\n",
       "        -0.57265629, -0.37796447,  0.23328474, -0.49829623, -0.44108109,\n",
       "        -0.07675374,  0.74540043,  0.19103213,  0.1381395 ,  0.15980461,\n",
       "         1.39960066,  0.79697229, -0.54469405, -0.45749531,  1.66302633],\n",
       "       [ 1.15470054, -0.6381449 ,  1.07304987, -1.75291964, -1.4284046 ,\n",
       "         1.73205081,  1.73205081, -0.45749379,  1.73205081,  1.67845629,\n",
       "        -0.57419571,  1.46345495,  2.64575131, -0.85692609, -1.46493936,\n",
       "         1.46345495,  2.64575131,  1.04978132,  1.10815131, -1.23502706,\n",
       "        -1.27162277,  1.09392467,  0.91648581, -2.15504527, -1.58256485,\n",
       "        -0.42312912, -1.64082531, -0.54469302, -0.45749552, -0.85398649],\n",
       "       [-0.57735027, -1.03084946, -1.37963555,  1.4832397 ,  1.52055974,\n",
       "        -0.57735027, -0.57735027,  0.15249817, -0.57735027, -1.18375082,\n",
       "         1.93138431, -0.57265629, -0.37796447, -0.85692609,  1.93750045,\n",
       "        -0.57265629, -0.37796447, -0.69985421, -1.03377874,  1.9407568 ,\n",
       "         1.6491682 , -0.33721475, -1.60608007,  1.0638465 ,  1.39603541,\n",
       "        -1.33449401,  0.79697229,  2.29718565,  0.15249841, -0.85398649],\n",
       "       [ 0.        , -0.24544035,  0.66426897, -1.21355975, -0.69116352,\n",
       "        -0.57735027, -0.57735027, -0.45749562, -0.57735027,  0.67138348,\n",
       "        -0.57419551, -0.57265629, -0.37796447, -0.64918643, -1.12469538,\n",
       "        -0.57265629, -0.37796447, -0.34992711,  0.45367269, -1.14681084,\n",
       "        -0.74056987,  0.29607453,  0.71665375, -1.00845288, -0.62366974,\n",
       "         0.10998517, -0.89073374, -0.54469349, -0.45749568, -0.13483997],\n",
       "       [-0.57735027,  0.53996876,  0.25548807,  0.40451992,  0.78331865,\n",
       "        -0.57735027, -0.57735027, -0.45749608, -0.57735027, -0.15629179,\n",
       "        -0.5741953 , -0.57265629, -0.37796447,  0.80499117,  0.50091475,\n",
       "        -0.57265629, -0.37796447, -0.69985421, -0.08180983,  0.17643244,\n",
       "         0.71982562, -0.33721475,  0.48030258,  0.5072598 ,  0.82222274,\n",
       "         0.78162973,  0.79697229, -0.54469373, -0.45749577,  0.58430655]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from IPython.display import display # For display(train.head()) equivalent\n",
    "\n",
    "# --- Define the Feature Engineering Function (as provided previously) ---\n",
    "def feature_engineering(df):\n",
    "    df_fe = df.copy()\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    df_fe['alone_to_social_ratio'] = df_fe['Time_spent_Alone'] / (df_fe['Social_event_attendance'] + epsilon)\n",
    "    df_fe['social_anxiety_score'] = df_fe['Drained_after_socializing_Yes'] + df_fe['Stage_fear_Yes']\n",
    "    df_fe['outside_per_post'] = df_fe['Going_outside'] / (df_fe['Post_frequency'] + epsilon)\n",
    "    df_fe['friends_to_social_events'] = df_fe['Friends_circle_size'] / (df_fe['Social_event_attendance'] + epsilon)\n",
    "    df_fe['alone_x_drained'] = df_fe['Time_spent_Alone'] * df_fe['Drained_after_socializing_Yes']\n",
    "    df_fe['stage_fear_x_outside'] = df_fe['Stage_fear_Yes'] * df_fe['Going_outside']\n",
    "    df_fe['social_events_x_friends'] = df_fe['Social_event_attendance'] * df_fe['Friends_circle_size']\n",
    "    df_fe['posts_x_friends'] = df_fe['Post_frequency'] * df_fe['Friends_circle_size']\n",
    "    df_fe['alone_x_social_anxiety'] = df_fe['Time_spent_Alone'] * df_fe['social_anxiety_score']\n",
    "    df_fe['outside_x_drained'] = df_fe['Going_outside'] * df_fe['Drained_after_socializing_Yes']\n",
    "    df_fe['Time_spent_Alone_sq'] = df_fe['Time_spent_Alone']**2\n",
    "    df_fe['Going_outside_sq'] = df_fe['Going_outside']**2\n",
    "    df_fe['Friends_circle_size_sq'] = df_fe['Friends_circle_size']**2\n",
    "    df_fe['Post_frequency_sq'] = df_fe['Post_frequency']**2\n",
    "\n",
    "    for col in ['Time_spent_Alone', 'Going_outside', 'Friends_circle_size', 'Post_frequency', 'Social_event_attendance']:\n",
    "        if col in df_fe.columns and (df_fe[col].min() >= 0).all() and df_fe[col][df_fe[col].notna()].std() > epsilon:\n",
    "             df_fe[f'{col}_log'] = np.log1p(df_fe[col])\n",
    "\n",
    "    df_fe['total_social_engagement'] = df_fe['Social_event_attendance'] + df_fe['Friends_circle_size'] + df_fe['Post_frequency']\n",
    "    df_fe['posts_per_event'] = df_fe['Post_frequency'] / (df_fe['Social_event_attendance'] + epsilon)\n",
    "    df_fe['alone_vs_outside'] = df_fe['Time_spent_Alone'] / (df_fe['Going_outside'] + epsilon)\n",
    "    df_fe['net_social_battery'] = df_fe['Social_event_attendance'] - df_fe['Drained_after_socializing_Yes']\n",
    "\n",
    "    return df_fe\n",
    "\n",
    "# --- Define the Simplified Preprocessor Pipeline Function (as provided previously) ---\n",
    "def simple_preprocessor_pipeline(X_df_train):\n",
    "    \"\"\"\n",
    "    Creates and fits a preprocessing pipeline (ColumnTransformer) based on column dtypes\n",
    "    from the training data. This handles imputation and scaling/encoding.\n",
    "\n",
    "    Args:\n",
    "        X_df_train (pd.DataFrame): The training DataFrame of features to fit the preprocessor on.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTransformer: A fitted ColumnTransformer object.\n",
    "    \"\"\"\n",
    "    numerical_cols = X_df_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = X_df_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    transformers = []\n",
    "\n",
    "    if numerical_cols:\n",
    "        transformers.append(('num_pipeline',\n",
    "                             Pipeline([\n",
    "                                 ('imputer', SimpleImputer(strategy='median')),\n",
    "                                 ('scaler', StandardScaler())\n",
    "                             ]),\n",
    "                             numerical_cols))\n",
    "\n",
    "    if categorical_cols:\n",
    "        transformers.append(('cat_pipeline',\n",
    "                             Pipeline([\n",
    "                                 ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                 ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "                             ]),\n",
    "                             categorical_cols))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    preprocessor.fit(X_df_train)\n",
    "    return preprocessor\n",
    "\n",
    "# --- Your Updated Code Block Starts Here ---\n",
    "\n",
    "# Assume 'train_df' is your initial raw DataFrame containing 'id' and 'Personality' (as string).\n",
    "# Example dummy train_df for demonstration (replace with your actual data loading)\n",
    "\n",
    "\n",
    "# 1. Initial Load, Drop 'id', Separate Target (y)\n",
    "if 'id' in train_df.columns:\n",
    "    X_features_only = train_df.drop(columns='id').copy()\n",
    "else:\n",
    "    X_features_only = train_df.copy()\n",
    "\n",
    "# Extract the target variable 'Personality'\n",
    "y = X_features_only['Personality']\n",
    "X_features_only = X_features_only.drop(columns='Personality') # Remove 'Personality' from features\n",
    "\n",
    "# 2. Encode the target variable (y) using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y) # This converts 'Extrovert'/'Introvert' to 0/1\n",
    "\n",
    "# 3. Apply Feature Engineering to the raw features\n",
    "X_features_engineered = feature_engineering(X_features_only)\n",
    "\n",
    "# 4. Split data into training and validation sets (now with engineered features)\n",
    "# y_encoded is used here as 'stratify' needs numerical values.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_features_engineered, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# 5. Fit the final preprocessor on X_train (this handles NaNs, scaling, etc. for ALL features)\n",
    "final_data_preprocessor = simple_preprocessor_pipeline(X_train)\n",
    "\n",
    "# 6. Transform both training and validation data using the fitted preprocessor\n",
    "X_train_final_processed = final_data_preprocessor.transform(X_train)\n",
    "X_val_final_processed = final_data_preprocessor.transform(X_val)\n",
    "\n",
    "# --- Equivalent to display(train.head()) but for the processed features ---\n",
    "# ColumnTransformer outputs a NumPy array. If you want a DataFrame with column names,\n",
    "# it's a bit more involved to get the exact names after OneHotEncoding, etc.\n",
    "# For simple inspection, you can display the first few rows of the NumPy array:\n",
    "print(\"First 5 rows of X_train_final_processed (NumPy array after all steps):\")\n",
    "display(X_train_final_processed[:5])\n",
    "\n",
    "# You now have:\n",
    "# X_train_final_processed (NumPy array): Your fully preprocessed and engineered training features\n",
    "# y_train (NumPy array): Your encoded training target\n",
    "# X_val_final_processed (NumPy array): Your fully preprocessed and engineered validation features\n",
    "# y_val (NumPy array): Your encoded validation target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9343cfd2-e8af-41d4-9b77-9d8042499b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Logistic Regression Model (Initial) ---\n",
      "\n",
      "Initial model training complete.\n",
      "\n",
      "--- Initial Model Evaluation on Validation Set ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "Accuracy: 1.0000\n",
      "ROC AUC Score: 1.0000\n",
      "\n",
      "--- Starting Hyperparameter Tuning with GridSearchCV ---\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Hyperparameter tuning complete.\n",
      "Best parameters found: {'C': 0.001, 'solver': 'liblinear'}\n",
      "Best cross-validation score (e.g., Accuracy): 1.0000\n",
      "\n",
      "--- Evaluation with Best Model on Validation Set ---\n",
      "Classification Report (Best Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "Accuracy (Best Model): 1.0000\n",
      "ROC AUC Score (Best Model): 1.0000\n",
      "\n",
      "--- Saving Preprocessor and Best Model ---\n",
      "Preprocessor saved to: final_data_preprocessor.joblib\n",
      "Best model saved to: best_logistic_regression_model.joblib\n",
      "\n",
      "Done with model creation, tuning, evaluation, and saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khaijones/.conda/envs/kaggle_introvert_extrovert_env/lib/python3.10/site-packages/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib # Add this line\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # GridSearchCV for tuning\n",
    "from sklearn.linear_model import LogisticRegression # Our model\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score # More evaluation metrics\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"--- Training Logistic Regression Model (Initial) ---\")\n",
    "\n",
    "model = LogisticRegression(random_state=42, solver='liblinear', C=1.0)\n",
    "model.fit(X_train_final_processed, y_train)\n",
    "\n",
    "print(\"\\nInitial model training complete.\")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val_final_processed)\n",
    "y_val_prob = model.predict_proba(X_val_final_processed)[:, 1] # Probabilities for ROC AUC\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\n--- Initial Model Evaluation on Validation Set ---\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_val, y_val_prob):.4f}\")\n",
    "\n",
    "# --- Hyperparameter Tuning using GridSearchCV ---\n",
    "\n",
    "print(\"\\n--- Starting Hyperparameter Tuning with GridSearchCV ---\")\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "# C: Inverse of regularization strength. Smaller C means stronger regularization.\n",
    "# Values are chosen to explore different regularization levels.\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear'] # Sticking with liblinear as it's good for small datasets\n",
    "    # Add other parameters like 'penalty': ['l1', 'l2'] if exploring different regularization types with liblinear\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "# estimator: The model to tune (LogisticRegression)\n",
    "# param_grid: The grid of hyperparameters to search\n",
    "# cv: Number of cross-validation folds (e.g., 5-fold cross-validation)\n",
    "# scoring: Metric to optimize (e.g., 'f1' is often good for imbalanced classes, 'accuracy' for balanced)\n",
    "# verbose: Controls the verbosity: 0 (silent), 1 (results on training), 2 (results on each fold)\n",
    "# n_jobs: Number of CPU cores to use. -1 means use all available cores.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    scoring='accuracy', # Or 'f1', 'roc_auc' depending on your primary metric\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_final_processed, y_train)\n",
    "\n",
    "print(\"\\nHyperparameter tuning complete.\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score (e.g., Accuracy): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# --- Re-evaluate with the Best Model on Validation Set ---\n",
    "print(\"\\n--- Evaluation with Best Model on Validation Set ---\")\n",
    "\n",
    "y_val_pred_best = best_model.predict(X_val_final_processed)\n",
    "y_val_prob_best = best_model.predict_proba(X_val_final_processed)[:, 1]\n",
    "\n",
    "print(\"Classification Report (Best Model):\")\n",
    "print(classification_report(y_val, y_val_pred_best))\n",
    "print(f\"Accuracy (Best Model): {accuracy_score(y_val, y_val_pred_best):.4f}\")\n",
    "print(f\"ROC AUC Score (Best Model): {roc_auc_score(y_val, y_val_prob_best):.4f}\")\n",
    "\n",
    "\n",
    "# --- Model Persistence (Saving the Preprocessor and the Best Model) ---\n",
    "\n",
    "print(\"\\n--- Saving Preprocessor and Best Model ---\")\n",
    "\n",
    "preprocessor_filename = 'final_data_preprocessor.joblib'\n",
    "model_filename = 'best_logistic_regression_model.joblib'\n",
    "\n",
    "# Save the fitted preprocessor\n",
    "joblib.dump(final_data_preprocessor, preprocessor_filename)\n",
    "print(f\"Preprocessor saved to: {preprocessor_filename}\")\n",
    "\n",
    "# Save the best trained model\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\"Best model saved to: {model_filename}\")\n",
    "\n",
    "print(\"\\nDone with model creation, tuning, evaluation, and saving.\")\n",
    "\n",
    "# --- How to Load and Use Later ---\n",
    "# print(\"\\n--- Example: Loading and Using Saved Model and Preprocessor ---\")\n",
    "# loaded_preprocessor = joblib.load(preprocessor_filename)\n",
    "# loaded_model = joblib.load(model_filename)\n",
    "\n",
    "# # Assume you have new_raw_data_df (e.g., test data or data for deployment)\n",
    "# # You need to apply the same steps as X_initial -> X_features_engineered\n",
    "# # Example:\n",
    "# # new_raw_data_df = pd.DataFrame(...) # Your new data\n",
    "# # if 'id' in new_raw_data_df.columns:\n",
    "# #     new_X_features_only = new_raw_data_df.drop(columns='id').copy()\n",
    "# # else:\n",
    "# #     new_X_features_only = new_raw_data_df.copy()\n",
    "# #\n",
    "# # # IMPORTANT: If 'Personality' column exists in new_raw_data_df, drop it here too\n",
    "# # if 'Personality' in new_X_features_only.columns:\n",
    "# #     new_X_features_only = new_X_features_only.drop(columns='Personality')\n",
    "# #\n",
    "# # new_X_features_engineered = feature_engineering(new_X_features_only)\n",
    "# # new_X_processed = loaded_preprocessor.transform(new_X_features_engineered)\n",
    "# # final_predictions = loaded_model.predict(new_X_processed)\n",
    "# # print(f\"Predictions for new data: {final_predictions[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b02dc0-6b6a-41fe-869d-520c1c928de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
