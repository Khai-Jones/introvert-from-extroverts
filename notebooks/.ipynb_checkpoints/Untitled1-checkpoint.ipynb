{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864393a9-b724-4560-b145-06965f0b0624",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;66;03m# NEW IMPORT for advanced tuning\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll libraries imported successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- Data Loading (No Changes) ---\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from IPython.display import display\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import optuna # NEW IMPORT for advanced tuning\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "\n",
    "# --- Data Loading (No Changes) ---\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/raw/train.csv')\n",
    "    test_df = pd.read_csv('../data/raw/test.csv')\n",
    "    sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "    print(\"\\nData loaded successfully!\")\n",
    "    print(f\"Train data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'train.csv', 'test.csv', and 'sample_submission.csv' are in the 'data/raw/' directory.\")\n",
    "    train_df = pd.DataFrame() # Avoid crashing script\n",
    "\n",
    "if not train_df.empty:\n",
    "    display(train_df.head())\n",
    "\n",
    "# --- IMPROVEMENT 1: Custom Transformer for Feature Engineering ---\n",
    "# This class wraps your feature engineering logic to be used within a scikit-learn Pipeline.\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # Nothing to learn during fitting, so we just return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = X.copy()\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        # --- All feature engineering logic is now inside this class ---\n",
    "        df['alone_to_social_ratio'] = df['Time_spent_Alone'] / (df['Social_event_attendance'] + epsilon)\n",
    "        df['social_anxiety_score'] = df['Drained_after_socializing'] + df['Stage_fear']\n",
    "        df['outside_per_post'] = df['Going_outside'] / (df['Post_frequency'] + epsilon)\n",
    "        df['friends_to_social_events'] = df['Friends_circle_size'] / (df['Social_event_attendance'] + epsilon)\n",
    "        df['alone_x_drained'] = df['Time_spent_Alone'] * df['Drained_after_socializing']\n",
    "        df['stage_fear_x_outside'] = df['Stage_fear'] * df['Going_outside']\n",
    "        df['social_events_x_friends'] = df['Social_event_attendance'] * df['Friends_circle_size']\n",
    "        df['posts_x_friends'] = df['Post_frequency'] * df['Friends_circle_size']\n",
    "        df['alone_x_social_anxiety'] = df['Time_spent_Alone'] * df['social_anxiety_score']\n",
    "        df['outside_x_drained'] = df['Going_outside'] * df['Drained_after_socializing']\n",
    "        df['Time_spent_Alone_sq'] = df['Time_spent_Alone']**2\n",
    "        df['Going_outside_sq'] = df['Going_outside']**2\n",
    "        df['Friends_circle_size_sq'] = df['Friends_circle_size']**2\n",
    "        df['Post_frequency_sq'] = df['Post_frequency']**2\n",
    "\n",
    "        for col in ['Time_spent_Alone', 'Going_outside', 'Friends_circle_size', 'Post_frequency', 'Social_event_attendance']:\n",
    "            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                 # The log transformation is applied within the pipeline\n",
    "                 df[f'{col}_log'] = np.log1p(df[col])\n",
    "\n",
    "        df['total_social_engagement'] = df['Social_event_attendance'] + df['Friends_circle_size'] + df['Post_frequency']\n",
    "        df['posts_per_event'] = df['Post_frequency'] / (df['Social_event_attendance'] + epsilon)\n",
    "        df['alone_vs_outside'] = df['Time_spent_Alone'] / (df['Going_outside'] + epsilon)\n",
    "        df['net_social_battery'] = df['Social_event_attendance'] - df['Drained_after_socializing']\n",
    "\n",
    "        return df\n",
    "\n",
    "# --- Data Preparation Workflow ---\n",
    "if 'id' in train_df.columns:\n",
    "    X_raw = train_df.drop(columns=['id', 'Personality']).copy()\n",
    "else:\n",
    "    X_raw = train_df.drop(columns=['Personality']).copy()\n",
    "\n",
    "y_raw = train_df['Personality']\n",
    "\n",
    "# --- Encode specific binary categorical features ('Yes'/'No') to numerical (0/1) ---\n",
    "# This is a prerequisite step before the pipeline\n",
    "binary_map = {'No': 0, 'Yes': 1}\n",
    "for col in ['Drained_after_socializing', 'Stage_fear']:\n",
    "    if col in X_raw.columns:\n",
    "        X_raw[col] = X_raw[col].map(binary_map).fillna(0) # Fill NaNs with 0 ('No')\n",
    "        X_raw[col] = pd.to_numeric(X_raw[col], errors='coerce')\n",
    "\n",
    "# Encode the target variable (y)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_raw)\n",
    "joblib.dump(le, 'label_encoder.joblib') # Save the label encoder\n",
    "\n",
    "# Split the RAW data. The pipeline will handle engineering and preprocessing.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_raw, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# --- Define the Preprocessor using column names from the *engineered* data ---\n",
    "# We do a one-time transform on the training data just to get the final column names for the preprocessor\n",
    "temp_feature_engineer = FeatureEngineer()\n",
    "X_train_engineered_temp = temp_feature_engineer.fit_transform(X_train)\n",
    "\n",
    "numerical_cols = X_train_engineered_temp.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X_train_engineered_temp.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Define the preprocessor that will be part of the main pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "\n",
    "# --- IMPROVEMENT 2: Create a Full Pipeline for XGBoost ---\n",
    "# This combines feature engineering, preprocessing, and the model into one object.\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost\n",
    "neg_count = pd.Series(y_train).value_counts()[0]\n",
    "pos_count = pd.Series(y_train).value_counts()[1]\n",
    "scale_pos_weight_value = neg_count / pos_count\n",
    "print(f\"Calculated scale_pos_weight for XGBoost (class 1): {scale_pos_weight_value:.2f}\\n\")\n",
    "\n",
    "\n",
    "# The master pipeline\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', FeatureEngineer()),\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=scale_pos_weight_value\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Hyperparameter Tuning for the Full XGBoost Pipeline ---\n",
    "print(\"--- Starting Hyperparameter Tuning for XGBoost Pipeline ---\")\n",
    "\n",
    "# Parameters must now reference the step name in the pipeline (e.g., 'classifier__n_estimators')\n",
    "param_grid_xgb_pipeline = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=xgb_pipeline, # Use the full pipeline here\n",
    "    param_grid=param_grid_xgb_pipeline,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the entire pipeline on the RAW training data\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nXGBoost Pipeline Hyperparameter tuning complete.\")\n",
    "print(f\"Best parameters found (XGBoost): {grid_search_xgb.best_params_}\")\n",
    "print(f\"Best cross-validation score (XGBoost - Accuracy): {grid_search_xgb.best_score_:.4f}\")\n",
    "\n",
    "# The best estimator is the entire fitted pipeline\n",
    "best_pipeline = grid_search_xgb.best_estimator_\n",
    "\n",
    "# --- IMPROVEMENT 3: Deeper Evaluation of the Best Model ---\n",
    "print(\"\\n--- Evaluation with Best XGBoost Pipeline on Validation Set ---\")\n",
    "y_val_pred_best_xgb = best_pipeline.predict(X_val)\n",
    "y_val_prob_best_xgb = best_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"Classification Report (Best XGBoost Pipeline):\")\n",
    "print(classification_report(y_val, y_val_pred_best_xgb, target_names=le.classes_))\n",
    "print(f\"Accuracy (Best XGBoost Pipeline): {accuracy_score(y_val, y_val_pred_best_xgb):.4f}\")\n",
    "print(f\"ROC AUC Score (Best XGBoost Pipeline): {roc_auc_score(y_val, y_val_prob_best_xgb):.4f}\")\n",
    "\n",
    "# --- NEW: Confusion Matrix Visualization ---\n",
    "print(\"\\n--- Generating Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_val, y_val_pred_best_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Best XGBoost Model')\n",
    "plt.show()\n",
    "\n",
    "# --- NEW: Feature Importance Plot ---\n",
    "print(\"\\n--- Generating Feature Importance Plot ---\")\n",
    "try:\n",
    "    # Get components from the best pipeline\n",
    "    feature_engineer = best_pipeline.named_steps['feature_engineering']\n",
    "    preprocessor = best_pipeline.named_steps['preprocessing']\n",
    "    classifier = best_pipeline.named_steps['classifier']\n",
    "\n",
    "    # Get feature names after all transformations\n",
    "    ohe_cols = preprocessor.named_transformers_['cat_pipeline']['onehot'].get_feature_names_out(categorical_cols)\n",
    "    all_final_features = numerical_cols + list(ohe_cols)\n",
    "\n",
    "    # Create and display the plot\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': all_final_features,\n",
    "        'importance': classifier.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(20) # Top 20\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=importance_df, palette='viridis')\n",
    "    plt.title('Top 20 Feature Importances from XGBoost Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot feature importances. This can be complex with pipelines. Error: {e}\")\n",
    "\n",
    "# --- Model Persistence ---\n",
    "print(\"\\n--- Saving Best Overall Model (Full Pipeline) ---\")\n",
    "# The object to save is the entire 'best_pipeline'\n",
    "model_filename = 'best_personality_pipeline.joblib'\n",
    "joblib.dump(best_pipeline, model_filename)\n",
    "print(f\"Best model pipeline saved to: {model_filename}\")\n",
    "\n",
    "print(\"\\nDone with model creation, tuning, evaluation, and saving.\")\n",
    "\n",
    "# --- IMPROVEMENT 4 (Optional): Advanced Tuning with Optuna ---\n",
    "# This section is for reference and can be run instead of GridSearchCV for more efficient tuning.\n",
    "#\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'classifier__n_estimators': trial.suggest_int('classifier__n_estimators', 100, 1000, step=50),\n",
    "#         'classifier__learning_rate': trial.suggest_float('classifier__learning_rate', 0.01, 0.3, log=True),\n",
    "#         'classifier__max_depth': trial.suggest_int('classifier__max_depth', 3, 10),\n",
    "#         'classifier__subsample': trial.suggest_float('classifier__subsample', 0.6, 1.0),\n",
    "#         'classifier__colsample_bytree': trial.suggest_float('classifier__colsample_bytree', 0.6, 1.0),\n",
    "#     }\n",
    "#     pipeline = xgb_pipeline.set_params(**params)\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "#     preds = pipeline.predict(X_val)\n",
    "#     accuracy = accuracy_score(y_val, preds)\n",
    "#     return accuracy\n",
    "#\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50) # Run 50 trials\n",
    "#\n",
    "# print(\"\\nOptuna study complete.\")\n",
    "# print(f\"Best validation accuracy: {study.best_value}\")\n",
    "# print(f\"Best params: {study.best_params}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727c74f-cf80-4130-aaf8-192d7ef346e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
